# Transformer-From-Attention-is-all-you-need
This is a personal project or say a module to implement Transformer architecture from scratch in reference to the research paper "Attention is all you need".
